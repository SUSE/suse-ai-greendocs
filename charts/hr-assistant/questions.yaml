questions:
- variable: otlpEndpoint
  label: "⚠️ OTLP Endpoint"
  type: string
  description: "Open Telemetry Endpoint - Include http:// and port. IMPORTANT: This setting is critical for error logging and observability. If pods are generating errors, verify this endpoint is correctly configured and accessible."
  required: true
  default: "http://opentelemetry-collector.observability.svc.cluster.local:4318"
  group: General


- variable: ollama.gpu.enabled
  group: General
  label: "Enable NVIDIA GPU for SUSE AI Engine"
  description: "Requires a node with NVIDIA drivers and the k8s device plugin installed."
  type: "boolean"
  default: true
  show_if: "ollama.enabled=true && ollama.hardware.type=nvidia"

- variable: ollama.hardware.type
  group: General
  label: "Target Hardware for SUSE AI Engine"
  description: "Select the target hardware architecture for the SUSE AI Engine deployment."
  type: "enum"
  required: true
  default: "nvidia"
  show_if: "ollama.enabled=true"
  options:
    - label: "NVIDIA GPU (amd64)"
      value: "nvidia"
    - label: "Apple Silicon / CPU (arm64)"
      value: "apple"

- variable: ollama.image.tag
  group: General
  label: "SUSE AI Engine Image Version"
  description: "The Docker image tag for the version of Ollama that will be installed from SUSE Application Collection (e.g., '0.11.4-10.88', '0.12.0-10.90')."
  type: "string"
  default: "0.11.4-10.88"
  show_if: "ollama.enabled=true"

- variable: ollama.enabled
  group: "Ollama Deployment"
  label: "Enable SUSE AI Engine Deployment"
  description: "If true, SUSE AI Engine will be deployed as part of this chart."
  type: "boolean"
  default: true

- variable: ollama.models.pull[0]
  group: "Ollama Deployment"
  label: "Primary AI Model"
  description: "The primary AI model to pull for HR queries (e.g., 'llama3.2:latest', 'tinyllama:latest')."
  type: "string"
  default: "llama3.2:latest"
  show_if: "ollama.enabled=true"

- variable: ollama.models.pull[1]
  group: "Ollama Deployment"
  label: "Secondary AI Model"
  description: "Additional model for embeddings (optional)."
  type: "string"
  default: "bge-large:latest"
  show_if: "ollama.enabled=true"

- variable: ollama.service.type
  group: "Ollama Deployment"
  label: "SUSE AI Engine Service Type"
  description: "Method to expose the SUSE AI Engine service. 'ClusterIP' is recommended."
  type: "enum"
  required: true
  default: "ClusterIP"
  show_if: "ollama.enabled=true"
  options:
    - "ClusterIP"
    - "NodePort"
    - "LoadBalancer"


- variable: ollama.persistence.enabled
  group: "Ollama Deployment"
  label: "Enable AI Model Persistence"
  description: "If true, a PersistentVolumeClaim will be created to store AI models."
  type: "boolean"
  default: false
  show_if: "ollama.enabled=true"

- variable: ollama.persistence.size
  group: "Ollama Deployment"
  label: "AI Model Storage Size"
  description: "The size of the PersistentVolumeClaim for AI models (e.g., '10Gi', '50Gi')."
  type: "string"
  default: "10Gi"
  show_if: "ollama.enabled=true && ollama.persistence.enabled=true"

- variable: ollama.persistence.storageClassName
  group: "Ollama Deployment"
  label: "AI Model Storage Class Name"
  description: "The storage class name for the AI models PVC. Leave empty to use the default storage class."
  type: "string"
  default: ""
  show_if: "ollama.enabled=true && ollama.persistence.enabled=true"

- variable: ollama.resources.requests.cpu
  group: "Ollama Deployment"
  label: "SUSE AI Engine CPU Request"
  description: "The amount of CPU to reserve for the SUSE AI Engine container (e.g., '500m', '1')."
  type: "string"
  default: "2"
  show_if: "ollama.enabled=true"

- variable: ollama.resources.requests.memory
  group: "Ollama Deployment"
  label: "SUSE AI Engine Memory Request"
  description: "The amount of memory to reserve for the SUSE AI Engine container (e.g., '1Gi', '2048Mi')."
  type: "string"
  default: "2Gi"
  show_if: "ollama.enabled=true"

- variable: ollama.resources.limits.cpu
  group: "Ollama Deployment"
  label: "SUSE AI Engine CPU Limit"
  description: "The maximum amount of CPU the SUSE AI Engine container can use."
  type: "string"
  default: "4"
  show_if: "ollama.enabled=true"

- variable: ollama.resources.limits.memory
  group: "Ollama Deployment"
  label: "SUSE AI Engine Memory Limit"
  description: "The maximum amount of memory the SUSE AI Engine container can use."
  type: "string"
  default: "16Gi"
  show_if: "ollama.enabled=true"


- variable: schedule
  label: "Schedule"
  type: string
  default: "*/5 * * * *"
  description: "Cron schedule for HR query simulation requests to demonstrate SUSE AI capabilities"
  required: true
  group: General
